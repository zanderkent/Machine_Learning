{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This was a notebook that I created for an application. \n",
    "I cannot share the data, but I can share my process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "# Helper: Early stopping.\n",
    "early_stopper = EarlyStopping(patience=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data that I formatted from a previous notebook and split it into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.7978\n",
       "1    0.2022\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import data saved from data cleaning\n",
    "X_train = pd.read_csv('~/Documents/statefarm_test/training_features.csv', sep='\\t')\n",
    "Y_train = pd.read_csv('~//Documents/statefarm_test/training_target.csv', sep='\\t')\n",
    "#Observe that is a class imbalance 80-20, should be fine. \n",
    "Y_train['y'].value_counts(normalize=True)\n",
    "#Y_train = pd.get_dummies(Y_train['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and compile model\n",
    "I created a few helper functions. One of them is to take a dictionary and create a model from it. The other one is to evaluate a model given to data. It validates on 25% of the data, shuffling every epoch to \"new\" validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_compile(network, input_shape, num_classes):\n",
    "    \"\"\"args is a dictionary\"\"\"\n",
    "    nb_layers = network['nb_layers']\n",
    "    nb_neurons = network['nb_neurons']\n",
    "    activation = network['activation']\n",
    "    optimizer = network['optimizer']\n",
    "    #Define the type of model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(nb_neurons, activation=activation, input_shape=input_shape, name='input_layer'))\n",
    "    model.add(Dropout(0.2, name='dropout_input'))\n",
    "    for i in range(nb_layers):\n",
    "        model.add(Dense(nb_neurons, activation=activation, name='layer_{}'.format(i+1)))\n",
    "        #Dropout to help avoid over-fitting\n",
    "        model.add(Dropout(0.2, name='dropout_{}'.format(i+1)))\n",
    "    # Output layer.\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def evaluate_model(network, X_train,Y_train):\n",
    "    input_shape = (X_train.shape[1],)\n",
    "    model = create_and_compile(network, input_shape,2)\n",
    "    early_stopper = EarlyStopping(patience=10)\n",
    "    model.fit(X_train, Y_train, verbose=0, epochs=100, validation_split=.25,callbacks=[early_stopper])\n",
    "    acc = model.evaluate(X_train,Y_train)[1]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brute force or finesse?\n",
    "Below is a brute force method for going through 672 different variations of the below.\n",
    "You can use random search, grid search, genetic algo or bayesian optimization for this. This was a quick and dirty way to do this on a small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_param_choices = {\n",
    "        'nb_layers': [1, 2, 3, 4],\n",
    "        'nb_neurons': [64, 128, 256, 512, 768, 1024],\n",
    "        'activation': ['relu', 'elu', 'tanh', 'sigmoid'],\n",
    "        'optimizer': ['rmsprop', 'adam', 'sgd', 'adagrad',\n",
    "                      'adadelta', 'adamax', 'nadam']\n",
    "    }\n",
    "import itertools\n",
    "keys, values = zip(*nn_param_choices.items())\n",
    "experiments = [dict(zip(keys, v)) for v in itertools.product(*values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = []\n",
    "for x in range(25):\n",
    "    network = random.choice(experiments)\n",
    "    print('Evaluating model number {}'.format(x+1))\n",
    "    new_list.append(evaluate_model(network, X_train,Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick the top performer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a helpful way to change out the values without messing with the overall flow\n",
    "#I only tried a few combinations of neurons, layers and functions before I got to above 95% accuracy\n",
    "network = {'nb_layers': 4, 'nb_neurons': 128, 'activation': 'elu', 'optimizer': 'rmsprop'}\n",
    "model = create_and_compile(network, (X_train.shape[1],), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0624 20:11:49.411870 140254335215424 deprecation.py:323] From /home/zander/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30000 samples, validate on 10000 samples\n",
      "Epoch 1/250\n",
      " - 11s - loss: 2.8402 - acc: 0.7744 - val_loss: 0.5668 - val_acc: 0.7266\n",
      "Epoch 2/250\n",
      " - 11s - loss: 0.4226 - acc: 0.8357 - val_loss: 0.2507 - val_acc: 0.9116\n",
      "Epoch 3/250\n",
      " - 11s - loss: 0.2290 - acc: 0.9224 - val_loss: 0.1427 - val_acc: 0.9517\n",
      "Epoch 4/250\n",
      " - 11s - loss: 0.1686 - acc: 0.9507 - val_loss: 0.1320 - val_acc: 0.9659\n",
      "Epoch 5/250\n",
      " - 11s - loss: 0.1495 - acc: 0.9621 - val_loss: 0.1143 - val_acc: 0.9687\n",
      "Epoch 6/250\n",
      " - 11s - loss: 0.1373 - acc: 0.9696 - val_loss: 0.1071 - val_acc: 0.9762\n",
      "Epoch 7/250\n",
      " - 11s - loss: 0.1348 - acc: 0.9717 - val_loss: 0.0870 - val_acc: 0.9816\n",
      "Epoch 8/250\n",
      " - 11s - loss: 0.1290 - acc: 0.9737 - val_loss: 0.1085 - val_acc: 0.9801\n",
      "Epoch 9/250\n",
      " - 12s - loss: 0.1331 - acc: 0.9752 - val_loss: 0.0917 - val_acc: 0.9837\n",
      "Epoch 10/250\n",
      " - 12s - loss: 0.1306 - acc: 0.9772 - val_loss: 0.1005 - val_acc: 0.9825\n",
      "Epoch 11/250\n",
      " - 11s - loss: 0.1331 - acc: 0.9796 - val_loss: 0.1126 - val_acc: 0.9801\n",
      "Epoch 12/250\n",
      " - 11s - loss: 0.1394 - acc: 0.9782 - val_loss: 0.1086 - val_acc: 0.9824\n",
      "Epoch 13/250\n",
      " - 11s - loss: 0.1310 - acc: 0.9810 - val_loss: 0.1239 - val_acc: 0.9846\n",
      "Epoch 14/250\n",
      " - 11s - loss: 0.1421 - acc: 0.9804 - val_loss: 0.1410 - val_acc: 0.9834\n",
      "Epoch 15/250\n",
      " - 11s - loss: 0.1483 - acc: 0.9800 - val_loss: 0.1155 - val_acc: 0.9849\n",
      "Epoch 16/250\n",
      " - 11s - loss: 0.1572 - acc: 0.9818 - val_loss: 0.1476 - val_acc: 0.9859\n",
      "Epoch 17/250\n",
      " - 11s - loss: 0.1486 - acc: 0.9818 - val_loss: 0.1156 - val_acc: 0.9859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8f0d41bd68>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train = pd.get_dummies(Y_train['y'])\n",
    "model.fit(X_train, Y_train, verbose=2, epochs=250, validation_split=.25,callbacks=[early_stopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 5s 114us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.990975"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_train,Y_train)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model for serving and re-loading\n",
    "model.save('model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
